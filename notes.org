#+TITLE: Draenor Project Notes
#+AUTHOR: Colin
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/home/colin/code/org-theme.css"/>

* Protocol Buffers

** Encoding

https://developers.google.com/protocol-buffers/docs/encoding

*** Example Message

#+BEGIN_SRC js
  message Test1 {
    required int32 a = 1;
  }
#+END_SRC

Setting /a/ to /150/ would output: /08 96 01/

*** Field Number + "Wire Type"

A key is a varint stored as: ~(field_number << 3) | wire_type~

See the link above for the wire types.

*** Analysis

/08/ is /000 1000/ with its msb removed.

- Right-most three bits gives the Wire Type: 0
- Right-shift by 3 gets the field number: 1

- /96 01/ becomes /1001 0110 0000 0001/
- Remove msbs: /001 0110 000 0001/
- Reverse groups of 7: /000 0001 001 0110/
- Mash together: /00000010010110/
- Reseparate: /1001 0110/ which is /150/

*** Strings (wire type 2)

#+BEGIN_SRC js
  message Test2 {
    required string b = 2;
  }
#+END_SRC

Setting /b/ to "testing" gives: /12 07 74 65 73 74 69 6e 67/

#+BEGIN_EXAMPLE
12                    // 0001 0010 (field #2, wire type 2)
07                    // "7 bytes to follow"
74 65 73 74 69 6e 67  // the UTF8 string "testing"
#+END_EXAMPLE

/07/ is the numbers of bytes of data that follow. In this case, each byte is
a byte of UTF8. The text is English, so we know each byte to correspond to
one letter.

*** Embedded Messages

#+BEGIN_SRC js
  message Test3 {
    requires Test1 c = 3;
  }
#+END_SRC

Once again setting /a/ to 150, we have: /1a 03 08 96 01/

Embedded messages are treated the same way as Strings (or any wire type 2).

#+BEGIN_EXAMPLE
1a        // 0001 1010 (field #3, wire type 2)
03        // "3 bytes to follow"
08 96 01  // varint 150
#+END_EXAMPLE

*** Optional and Repeated Elements
Anything marked ~repeated~ (but without ~[packed=true]~) just appears as
zero or more key-value pairs with the same tag number.

Anything marked ~optional~ simply may or may not be present.

From the Protobuf site:

#+BEGIN_QUOTE
Normally, an encoded message would never have more than one instance of a
non-repeated field. However, parsers are expected to handle the case in
which they do.
#+END_QUOTE

We can go faster if we ignore this condition, and assume each field will
only appear once.

**** Packed Repeated Fields

If the repeated list is empty, this field is not present in the encoded
message.

#+BEGIN_SRC js
  message Test 4 {
    repeated int32 d = 4 [packed=true];
  }
#+END_SRC

Giving /d/ 3, 270, and 86942, we get:

/22 06 03 8E 02 9E A7 05/

#+BEGIN_EXAMPLE
22        // 0010 0010 - tag (field number 4, wire type 2)
06        // 0000 0110 - payload size (6 bytes)
03        // 0000 0011 - first element (varint 3)
8E 02     // 1000 1110 0000 0010 - second element (varint 270)
9E A7 05  // 1001 1110 1010 0111 0000 0101 - third element (varint 86942)
#+END_EXAMPLE

*** Field Order

From the Protobuf spec:

#+BEGIN_QUOTE
...when a message is serialized its known fields should be written
sequentially by field number ... This allows parsing code to use
optimizations that rely on field numbers being in sequence. However,
protocol buffer parsers must be able to parse fields in any order ...
#+END_QUOTE

Our parser can be written pretty cleanly if we ignore this warning and
assume that the data is always sorted by field number.

* OSM PBF Format

Explanation of the spec: https://wiki.openstreetmap.org/wiki/PBF_Format

Official /.proto/ files: https://github.com/scrosby/OSM-binary/tree/master/src

A /.osm.pbf/ is a repeating sequence of:

- An /int4/ length of the ~BlobHeader~ message
- A serialized ~BlobHeader~
- The header's corresponding ~Blob~

** Concepts

*** BlobHeader

#+BEGIN_SRC C
  message BlobHeader {
          /* Either "OSMHeader" or "OSMData" */
          required string type = 1;

          /* Some optional metadata (bbox, etc.) */
          optional bytes indexdata = 2;

          /* The number of bytes in the subsequent `Blob` */
          required int32 datasize = 3;
  }
#+END_SRC

Every fileblock must have a ~Blob~ labelled as a ~OSMHeader~ before its
blocks labelled ~OSMData~. Note:

#+BEGIN_QUOTE
Parsers should ignore and skip fileblock types that they do not recognize.
#+END_QUOTE

Since we would know the size of the ~Blob~ from its header, skipping
unrecognized ones should be easy. Should we include logic for that in
~streaming-osm~? I suppose it wouldn't hurt, given that we'd already be
pattern matching on ~OSMHeader~ and ~OSMData~ already.

*** Blob

#+BEGIN_SRC C
  message Blob {
          /* --- UNCOMPRESSED DATA --- */
          optional bytes raw = 1;  // Should be less than 16mb!

          /* --- COMPRESSED DATA --- */
          optional int32 raw_size = 2;  // uncompressed size if compressed
          optional bytes zlib_data = 3;

          /* --- UNUSED FIELDS, CAN IGNORE --- */
          optional bytes lzma_data = 4;
          optional bytes OBSOLETE_bzip2_data = 5 [deprecated=true];
  }
#+END_SRC

Uncompressed, any ~Blob~ /should/ contain less than 16mb, and /must/ contain
less than 32mb of data. Any given blob with either have only the /1/ field,
or both /2/ and /3/ together.

We /might/ be able to ignore ~raw_size~ too. It seems to be only for
verifying ~Blob~ sizes.

*** HeaderBlock

The contents of a ~Blob~ if its type was marked as ~OSMHeader~.

#+BEGIN_SRC C
  message HeaderBlock {
          /* Can be:
           ,*  - OsmSchema-V0.6
           ,*  - DenseNodes
           ,*  - HistoricalInformation
           ,*
           ,* The idea being that a serialized file will declare how robust
           ,* of a parser it needs, and parsers can reject files that
           ,* ask for things they can't decode.
           ,*/
          repeated string required_features = 4;

          /* --- IGNORABLE --- */
          optional HeaderBBox bbox = 1; /* The bounding box of a Block */
          repeated string optional_features = 5;
          optional string writingprogram = 16;
          optional string source = 17;
          optional int64 osmosis_replication_timestamp = 32;
          optional int64 osmosis_replication_sequence_number = 33;
          optional string osmosis_replication_base_url = 34;
  }
#+END_SRC

*** HeaderBBox

Doesn't matter. We know how many bytes it would be because of how embedded
messages are encoded, so we can just ~take~ that many bytes and dump them.

*** PrimitiveBlock

The contents of a ~Blob~ if its type was marked as ~OSMData~. We should
expect 8,000 OSM Elements to be packed into one of these, but there could be
more such that the 16mb rule stated above is obeyed.

#+BEGIN_SRC C
  message PrimitiveBlock {
          /* Metadata table for the entire block */
          required StringTable stringtable = 1;

          /* The actual Element data */
          repeated PrimitiveGroup primitivegroup = 2;

          /* Necessary for restoring "real" LatLng values. The values stored
           ,* in `PrimitiveGroup` are delta encoded (etc.) to save space.
           ,*/
          optional int32 granularity = 17 [default=100];
          optional int64 lat_offset = 19 [default=0];
          optional int64 lon_offset = 20 [default=0];

          // Do we care about this?
          optional int32 date_granularity = 18 [default=1000];
  }
#+END_SRC

This conversion is important:

#+BEGIN_EXAMPLE
latitude = .000000001 * (lat_offset + (granularity * lat))
longitude = .000000001 * (lon_offset + (granularity * lon))
#+END_EXAMPLE

This must be performed on every LatLng value we decode. But be careful of
numeric conversion! ~lat_offset~, for instance, is an ~int64~.

*** StringTable

All strings found in any Element in this block are made into a unique set,
and then ordered into a list. The encoded Elements then just store indexes
into this table, kind of like VectorTiles (or perhaps it's VTs who got this
idea from OSM.)

#+BEGIN_SRC C
  message StringTable {
          /* Index 0 is always blank and unused, apparently. */
          repeated bytes s = 1;
  }
#+END_SRC

The beginning of the encoded ~StringTable~ will always look like this:

#+BEGIN_EXAMPLE
0a    // S 1 'stringtable'
d4 2e // length 5972 bytes
0a    // S 1
00    // length 0 bytes
0a    // S 1
07    // length 7 bytes
etc.
#+END_EXAMPLE

Notice how no data bytes follow the ~00~; another tag-byte comes
immediately.

*** PrimitiveGroup

~PrimitiveGroup~ is ~repeated~ within ~PrimitiveBlock~, and we can expected
one Group for each Element type. Element types are *never* to mix in the
same Group (i.e. ~PrimitiveGroups~ are homogeneous).

#+BEGIN_SRC C
  message PrimitiveGroup {
          repeated Node       nodes = 1;
          optional DenseNodes dense = 2;
          repeated Way        ways = 3;
          repeated Relation   relations = 4;

          /* --- UNUSED, CAN IGNORE --- */
          repeated ChangeSet  changesets = 5;
  }
#+END_SRC

*** Node

#+BEGIN_SRC C
  message Node {
          required sint64 id = 1;

          /* Indices into the `StringTable`. These are "tag" metadata. */
          repeated uint32 keys = 2 [packed = true];
          repeated uint32 vals = 3 [packed = true];

          /* The Element metadata, like changeset, timestamp etc. */
          optional Info info = 4;

          /* Location of the Node */
          required sint64 lat = 8;
          required sint64 lon = 9;
  }
#+END_SRC

*** DenseNodes

Delta-encoded Nodes, taking advantage of the fact that Nodes with nearby IDs
will often be geographically near to each other as well.

#+BEGIN_SRC C
  message DenseNodes {
          /* Delta-encoded parallel arrays */
          repeated sint64 id  = 1 [packed = true];
          repeated sint64 lat = 8 [packed = true];
          repeated sint64 lon = 9 [packed = true];

          /* Metadata */
          optional DenseInfo denseinfo = 5;

          /* [key, val, key, val, 0, key, val, 0, ...] */
          repeated int32 keys_vals = 10 [packed = true];
   }
#+END_SRC

In these, indices into the ~StringTable~ are stored in one array. ~0~ is
used to separate one Node's tags from the next. /Caveat:/ if no nodes here
have tags, then this list will be empty. Does that mean the
field-num/wire-type will still be encoded, with a byte count of 0?

*** Way

#+BEGIN_SRC C
  message Way {
          required int64 id = 1;

          /* Indices into the `StringTable`, same as Nodes */
          repeated uint32 keys = 2 [packed = true];
          repeated uint32 vals = 3 [packed = true];

          optional Info info = 4;

          /* Delta-encoded refs to this Way's Nodes */
          repeated sint64 refs = 8 [packed = true];
  }
#+END_SRC

*** Relation

#+BEGIN_SRC C
  message Relation {
          enum MemberType {
                  NODE = 0;
                  WAY = 1;
                  RELATION = 2;
          }
          required int64 id = 1;

          /* Same as Nodes and Ways */
          repeated uint32 keys = 2 [packed = true];
          repeated uint32 vals = 3 [packed = true];

          optional Info info = 4;

          /* Indices into the `StringTable` to find the "role" value */
          repeated int32 roles_sid = 8 [packed = true];

          /* IDs of the Relation's children */
          repeated sint64 memids = 9 [packed = true]; // DELTA encoded

          /* Types of the child Elements */
          repeated MemberType types = 10 [packed = true];
  }
#+END_SRC

*** Info

Non-geographic metadata.

#+BEGIN_SRC C
  message Info {
          optional int32 version = 1 [default = -1];
          optional int32 timestamp = 2;
          optional int64 changeset = 3;
          optional int32 uid = 4;

          /* Index into the `StringTable` to find the username */
          optional int32 user_sid = 5;

          optional bool visible = 6;
  }
#+END_SRC

~timestamp~ here needs to adjusted by the ~date_granularity~ found in the
parent ~PrimitiveBlock~ by the following formula:

#+BEGIN_EXAMPLE
millisec_stamp = timestamp * date_granularity
#+END_EXAMPLE

Where the final result is the number of millis since the 1970 epoch.

*** DenseInfo

#+BEGIN_SRC C
  message DenseInfo {
          /* Delta-encoded parallel arrays */
          repeated int32 version = 1 [packed = true];  // Not delta encoded
          repeated sint64 timestamp = 2 [packed = true];
          repeated sint64 changeset = 3 [packed = true];
          repeated sint32 uid = 4 [packed = true];
          repeated sint32 user_sid = 5 [packed = true];

          repeated bool visible = 6 [packed = true];
  }
#+END_SRC

* streaming-osm
** Parsing

During parsing, we'll often run into int values that tell us how many bytes
forward to expect to belong to the current "block" (chars in a string,
repeated packed values, etc.) that we're parsing. /Attoparsec/ gives us this:

#+BEGIN_SRC haskell
  take :: Int -> Parser ByteString
#+END_SRC

How do we extract the final values from the ~ByteString~? If we use ~take~,
the entire ~ByteString~ will be brought into memory. Is that bad? It might
not be, depending on how long we expect such fields to be in the specific
case of OSM, and also the /type/ of value we hope to parse.

*** Strings

UTF8 might seem like it presents a challenge, but really this is the easy
case. Interpretting any given ~ByteString~ as UTF8 text is trivial with the
~bytestring~ library. In the case of a field that we know to be a String, we
can use ~take~ above and marshall the result directly to a ~Text~ value (or
just leave it as a ~ByteString~? That would be fastest. Besides, we might be
rewriting it again right away.)

*** Packed Repeated Fields

Only primitive numeric types can be declared as /packed/. That means they're
all just stored as /varints/, so we have to test the leading bit of each
byte to determine the break point between different values. I attempted a
~groupBytes~ function previously, but ~ByteString.groupBy~ did not behave as
I expected it to.

*** Embedded Messages

Off the top of my head, it seems like you can ignore the "following bytes"
number given, and just parse what you expect to be there via the parser of
the Message that happens to be embedded. Will we ever overrun and gobble too
many bytes this way?
** Optimization

https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html

*** Preamble

Alright! The parser has been tested on files as big as Vancouver (6.2mb) and
it can parse out all the Elements successfully. Some notes before we start
benchmarking / optimizing in earnest:

- Performance has been observed to be linear with file size. This means that
  I can bench small files and accurately guess run times for larger files.
- Ways and relations ~count~ faster than nodes. This means laziness /
  streaming are doing good things and ignoring ~Node~ data when it isn't
  needed.
- Initially, Uku ~Node~ data parses and counts in 71ms. My first goal is to
  improve this by an order of magnitude. The ~osm4scala~ library can count
  all of Ireland's Elements in 8.9 seconds.

*** Results

/File sizes:/

- Uku: 128kb
- Vancouver: 6.2mb
- Ireland: 133.4mb
- Planet: 35gb

| Optimizations        | Nodes (ms) | Ways (ms) | Relations (ms) | Vancouver (ms) | Ireland (s) | Planet (mins) |
|----------------------+------------+-----------+----------------+----------------+-------------+---------------|
| None                 |      68.43 |      4.36 |           4.25 |       3394.128 |   73.028496 |     327.00416 |
| +Lazy Attoparsec+      |       67.8 |      4.56 |           4.25 |        3362.88 |    72.35616 |      323.9936 |
| +Streaming Decompress+ |      70.12 |       4.5 |           4.41 |       3477.952 |   74.832064 |     335.08011 |
| -O2                  |      59.63 |       4.2 |            3.9 |       2957.648 |   63.637136 |     284.95189 |
| Ignore ~HeaderBlock~   |       58.3 |      4.08 |           3.75 |        2891.68 |    62.21776 |     278.59627 |
| Use ~INLINE~           |       57.5 |      3.79 |           3.59 |          2852. |      61.364 |     274.77333 |
| Strict ~many1~         |      46.61 |      3.75 |           3.54 |       2311.856 |   49.742192 |     222.73365 |
| Strict ~Map~           |      46.35 |      3.77 |           3.56 |        2298.96 |    49.46472 |      221.4912 |
| +Strict State+         |      52.46 |      3.79 |           3.67 |       2602.016 |   55.985312 |     250.68885 |
| +StrictData Ext.+      |      68.55 |     28.66 |           27.8 |        3400.08 |    73.15656 |      327.5776 |
| Use ~scanl~            |      31.79 |      3.82 |           3.63 |       1576.784 |   33.926288 |     151.91381 |
| ~Int~ everywhere       |       25.8 |      3.55 |           3.52 |        1279.68 |    27.53376 |      123.2896 |
| *TARGET*               |        6.5 |           |                |          322.4 |      6.9368 |     31.061333 |
#+TBLFM: $5=$2 * 49.6::$6=($2 * 1067.2) / 1000::$7=($2 * 286720) / 60000

| Concurrency  | Concurrent Tasks | Nodes (s) |
|--------------+------------------+-----------|
| None         | N/A              |        24 |
| Con - 1 core | 4                |      26.1 |
| Con - 4 core | 4                |      31.7 |
| Con - 4 core | 8                |      33.5 |
| Con - 4 core | 16               |      38.4 |

For comparison, it can sequentially count the number of ~Blob~ in 0.001s,
and parse those to ~Block~ in 0.04s.

*** Lazy Attoparsec

~BL.fromStrict~ is /O(1)/, but ~BL.toStrict~ is /O(n)/ and heavily warned
against in the ~bytestring~ documentation. Attoparsec can run parsers across
lazy bytestrings as well, which lets us avoid the reverse conversion.

#+BEGIN_SRC haskell
  blocks :: Stream (Of Blob) RIO () -> Stream (Of Block) RIO ()
  blocks = S.concat . S.map f
    where f (Blob (Left bs)) = A.parseOnly block bs
          f (Blob (Right (_, bs))) = AL.eitherResult . AL.parse block . decompress $ BL.fromStrict bs
#+END_SRC
*** Streaming Decompress

This avoids conversion between ~ByteString~ types completely, but still
manages to be slower. I think streaming decompress just isn't necessary
here, considering how small we know each of the file blocks to be. If the
/entire/ file was compressed, then yes, this would be the correct approach.

#+BEGIN_SRC haskell
  blocks :: Stream (Of Blob) RIO () -> Stream (Of Block) RIO ()
  blocks = flip S.for f
    where f (Blob (Left bs)) = either (const $ pure ()) S.yield $ A.parseOnly block bs
          f (Blob (Right (_, bs))) = void . A.parsed block . Z.decompress Z.defaultWindowBits $ Q.fromStrict bs
#+END_SRC

*** Ignore ~HeaderBlock~

It's not referenced anyway, so why try to parse it?

#+BEGIN_SRC haskell
  header :: A.Parser ()
  header = do
    A.take 4
    A.word8 0x0a
    A.anyWord8
    A.string "OSMHeader" <|> A.string "OSMData"
    optional (A.word8 0x12 *> varint >>= advance)
    void (A.word8 0x18 *> varint @Int)

  advance :: Int -> A.Parser ()
  advance n = T.Parser $ \t pos more _lose suc -> suc t (pos + T.Pos n) more ()
  {-# INLINE advance #-}
#+END_SRC

*** Use ~Inline~

~INLINE~ is a bit more aggressive than ~INLINABLE~, and we gain a little
speed up if we insist this to the compiler.

*** Strict ~many1~

The strict variant of ~many1~ (adds an apostrophe) forces its return value
to WHNF, which appears to have a very significant impact. I use it in a
number of places so this change bought a good speed up. Is there anywhere
else I'm being "too lazy"?

*** Strict ~Map~

From the ~Data.Map~ module documentation:

#+BEGIN_QUOTE
Note: You should use ~Data.Map.Strict~ instead of this module if:

- You will eventually need all the values stored.
- The stored values don't represent large virtual data structures to be lazily computed.
#+END_QUOTE

The improvement is slight, as ~Map~ is only used in retrieving tags from
~DenseNodes~.
*** Strict ~State~

It /looks/ like this is slowing the benchmarks down, but properly profiled,
this actually reduces overall CPU time and memory usage by ~undelta~ by
quite a bit. Overall the change to Strict State should be better in the real
world.

*** ~StrictData~ Extension

Sometimes it can be handy to add this extension to a ~Types.hs~ file:

#+BEGIN_SRC haskell
  {-# LANGUAGE StrictData #-}
#+END_SRC

This was not one of those cases.

*** Use ~scanl~

The strict ~scanl~ (with an apostrophe) to be precise. This made ~undelta~
/way/ faster, which seemed to be a decent bottleneck. This is also confuses
the profiling results: they said that undelta only took 1.4% of the total
CPU time, yet speeding it up gave a 30% /overall/ speedup to the benchmarks.
I guess I should be skeptical of both sources of info, and optimize where it
makes sense?
*** ~Int~ everywhere

It turned out to be possible to specialize almost everything to ~Int~,
avoiding generic functions. This, plus allowing only a single ~INLINE~ of
~varint~ accounted for the most speedup. Further inlines didn't seem to
help, or made speed worse.
* July Wrap-up

** Overview

As of this month, version ~1.0.0~ of ~streaming-osm~ was released. It provides
a reliable method for parsing ~.osm.pbf~ files in Haskell, and does so in a /streaming/
fashion so that there is very little RAM overhead regardless of input file size.

For all intents and purposes, the library works and is fast. However, despite
significant effort in micro-optimization, it is not fast enough.

** Results

My goal was to be able to parse and count the number of ~Node~ elements in
the full /OSM Planet/ file (~35gb) in half an hour. If just counting took 30 minutes,
then fully processing the entire file and rewriting into a ~.orc~ file should then take
around an hour (at least). This was a good target, as Seth Fitzsimmons reported that his
original ~osm2orc~ program could do the entire task in 4 hours using preexisting Java
libraries of essentially the same function.

In order to actually measure how my library was performing, I could luckily avoid
parsing the actual 35gb file and stick to reading smaller files, timing the parse,
and extrapolating. This is because of the /streaming/ approach (and the way OSM PBF data
is formatted) - parsing time is /linear/ with file size. The initial, unoptimized code
was far from fast:

| Nodes (ms) | Ways (ms) | Relations (ms) | Planet (mins) |
|------------+-----------+----------------+---------------|
|      68.43 |      4.36 |           4.25 |     327.00416 |

6.5 hours, just to count the Nodes! No thank you. This spurred me into the deepest
research into performance Haskell that I've yet done, and I learned a lot. Namely:

- That ~-O2~ is our friend
- The proper use of ~INLINE~ and /phase control/ numbers
- The value of /fold/ operations that are strict in the accumulator
- What ~scanl~ is for (hint: fast code)
- That laziness works

After applying every technique I found, I brought the Node counting time down to:

| Nodes (ms) | Ways (ms) | Relations (ms) | Planet (mins) |
|------------+-----------+----------------+---------------|
|       25.8 |      3.55 |           3.52 |      123.2896 |

2 hours. A lot faster, but not enough. It seemed like this was as fast as single-core
parsing could get. Ah, then why not multicore? OSM divides their Element data into blocks
of 8000, requiring a two-stage parsing process. Why not grab each block of raw bytes, then
parse the real meat in parallel? I tried to employ the brand-new
~streaming-concurrency~ library with the direct help of the author, but after
some benchmarks it was clear to the both of us that there was almost no time gain to
be had with his library. Yes the results became concurrent, but the underpinnings didn't
seem to provide the advertised benefit. This is something that should be revisited in
future, however, since I think the concurrent approach is sound in general.

** Next Steps

I learned a lot about performance Haskell, as well as how Protobuf works. I hand-wrote
the byte parser instead of using any existing protobuf libraries because I thought that
would get me the most speed. I could make assumptions, cheat, leave things out, and
hopefully pull ahead of existing tools. That didn't happen in the end, but I'm still
hopeful. Should ~streaming-concurrency~ recieve a performance update, then my
~streaming-osm~ can benefit for free.

For now, there isn't a point in continuing the other aspects of my multi-part project
to convert PBF data to ORC via Haskell. If this first piece (~streaming-osm~) can't
get fast enough, then adding the extra logic of ORC conversion won't help the total time.
That said, the research should be considered a success since much was gained. I learned
a lot that can be applied elsewhere, and I released a new, useful library.

While I'm pausing the rest of the project for now, there are some follow-up tasks:

- Benchmark ~streaming-osm~ with the new GHC 8.2.1
- Benchmark ~streaming-osm~ with an updated ~streaming-concurrency~ (maybe never)

If these can bring the time down to where it needs to be, then the project should
be continued. Until then, I will pursue other ventures.
